<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="None">
  
  <link rel="shortcut icon" href="img/favicon.ico">
  <title>Home - Project Report - Internship</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="css/theme.css" type="text/css" />
  <link rel="stylesheet" href="css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Home";
    var mkdocs_page_input_path = "index.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="js/jquery-2.1.1.min.js" defer></script>
  <script src="js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> Project Report - Internship</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1 current">
		
    <a class="current" href=".">Home</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#project-report-internship">Project Report - Internship</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#november">November</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#references">References</a></li>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="datasets/">Datasets</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="codes/">Codes</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">Project Report - Internship</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>Home</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="project-report-internship">Project Report - Internship</h1>
<p><a href="datasets/">[Datasets]</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="codes/">[Codes]</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="">[ToDo]</a></p>
<p><Description></p>
<h2 id="november">November</h2>
<h3 id="week-1">Week 1</h3>
<h4 id="two-tracks-for-the-project">Two tracks for the project</h4>
<ol>
<li>Generating Face Depth Images given RGB and IR image:</li>
</ol>
<p>Improving depth estimation of faces by providing both RGB and IR image to the GAN.
[Results]
This could be a Siamese network , with two channels to the generator.
Experiment with different loss function.
Introduce labels (gender, age etc.) to Discriminator in-order to <em>improve</em> results.
Experiment with different IR-patterns.
Evaluating Depth output (from RGB-DEPTH paper)
Test ir2depth, binarized IR images, varied internsity and varied depth
 Use IR-GAN as training weights for Gender_CNN : Train Generator with discriminator using only labels and not depth.
[ WISGâ€™18  [1] [2] ]</p>
<ol>
<li>Face analysis from raw data:</li>
</ol>
<p>Add more layers to the CNN architecture / Transfer learning.
Fab-net [learn more]
Teacher student network.</p>
<p><br></p>
<ul>
<li>
<p><span style="color: #00000; font-family: Babas; font-size: 1.5em;">Retrained on binarized IR images</span></p>
<p><strong><pre>Non-binarized IR input               Training loss</pre></strong></p>
<p><pre><img src="img/old_ir.png" alt="drawing" width="250"/>      <img src="img/old_loss.png" alt="drawing" width="300"/></pre></p>
<p><a href="results/v1/">[Test output results (link)]</a></p>
<p><strong><pre>Binarized IR input               Training loss</pre></strong></p>
<p><pre><img src="img/new_ir.png" alt="drawing" width="250"/>  <img src="img/new_loss.png" alt="drawing" width="400"/></pre></p>
<p><a href="results/v3/">[Test output results (link)]</a></p>
<p><strong><pre>Trained on binarized IR images with varying distance from camera</pre></strong></p>
<p><pre><img src="results/v2/images/100_real_A.png" alt="drawing" width="200"/> <img src="results/v2/images/101_real_A.png" alt="drawing" width="200"/> <img src="results/v2/images/102_real_A.png" alt="drawing" width="200"/></pre></p>
<p><a href="results/v2/">[Test output results (link)]</a></p>
<p><strong><pre>Tested on unseen pose and orientation </pre></strong></p>
<p><pre><img src="img/new_ir_new.png" alt="drawing" width="550"/></pre></p>
</li>
</ul>
<p><br></p>
<ul>
<li>
<p><span style="color: #00000; font-family: Babas; font-size: 1.5em;">Evaluation metrics</span></p>
<ul>
<li>
<p><strong>RMSE</strong>:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="img/rmse.png" alt="drawing" width="250"/></p>
</li>
<li>
<p><strong>log<sub>10</sub>error</strong>:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="img/log10.png" alt="drawing" width="330"/></p>
</li>
<li>
<p><strong>Structural similarity index (SSIM)</strong> <sup><a href="#references">[10]</a></sup>: <em>While metrics such as MSE estimate absolute errors, SSIM is a perceptionbased metric that considers image degradation as perceived change in structural information, while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms. Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in a visual rendering of a scene. Using this metric, we retain the original 2D structure of the image (as opposed to using a vector notation) since SSIM is computed on windows of images.</em><sup><a href="#references">[9]</a></sup></p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Real</th>
<th align="center">Fake</th>
<th align="right">SSIM error</th>
<th align="right">log<sub>10</sub>error</th>
<th align="right">RMSE</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="img/irv2/1_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv2/1_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.98 <br> <img src="img/irv2/1_error.png" alt="drawing" width="110"/></td>
<td align="right">0.0174</td>
<td align="right">9.0051</td>
</tr>
<tr>
<td><img src="img/irv2/110_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv2/110_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.973 <br> <img src="img/irv2/110_error.png" alt="drawing" width="110"/></td>
<td align="right">0.0205</td>
<td align="right">9.5557</td>
</tr>
<tr>
<td><img src="img/irv3/1_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv3/1_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.978 <br> <img src="img/irv3/1_error.png" alt="drawing" width="110"/></td>
<td align="right">0.0103</td>
<td align="right">5.1186</td>
</tr>
<tr>
<td><img src="img/irv3/101_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv3/101_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.983 <br> <img src="img/irv3/101_error.png" alt="drawing" width="110"/></td>
<td align="right">0.0122</td>
<td align="right">3.9700</td>
</tr>
<tr>
<td><img src="img/irv3/110_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv3/110_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.986 <br> <img src="img/irv3/110_error.png" alt="drawing" width="110"/></td>
<td align="right">0.0100</td>
<td align="right">3.0364</td>
</tr>
<tr>
<td><img src="img/irv3/141_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv3/141_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.991 <br> <img src="img/irv3/141_error.png" alt="drawing" width="110"/></td>
<td align="right">0.0043</td>
<td align="right">5.7457</td>
</tr>
<tr>
<td><img src="img/irv4/1_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv4/1_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.972 <br> <img src="img/irv4/1_error.png" alt="drawing" width="110"/></td>
<td align="right">0.0152</td>
<td align="right">8.3283</td>
</tr>
<tr>
<td><img src="img/irv4/110_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv4/110_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.968 <br> <img src="img/irv4/110_error.png" alt="drawing" width="110"/></td>
<td align="right">0.0181</td>
<td align="right">9.3406</td>
</tr>
<tr>
<td><img src="img/irv4/133_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv4/133_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.906 <br> <img src="img/irv4/133_error.png" alt="drawing" width="110"/></td>
<td align="right">0.0856</td>
<td align="right">30.3137</td>
</tr>
<tr>
<td><img src="img/irv4/134_real_B.png" alt="drawing" width="110"/></td>
<td align="center"><img src="img/irv4/134_fake_B.png" alt="drawing" width="110"/></td>
<td align="right">0.854 <br> <img src="img/irv4/134_error.png" alt="drawing" width="110"/></td>
<td align="right">0.1186</td>
<td align="right">30.8789</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>RANSAC</strong>: <em>For a quantitative comparison, we used the first 200 subjects from the BU-3DFE dataset, which contains facial images aligned with ground truth depth images. Each method provides its own estimation for the depth image alongside a binary mask, representing the valid pixels to be taken into account in the evaluation. Obviously, since the problem of reconstructing depth from a single image is ill-posed, the estimation needs to be judged up to global scaling and transition along the depth axis. Thus, we compute these paramters using the Random Sample Concensus (RANSAC) approach, for normalizing the estimation according to the ground truth depth. This significantly reduces the absolute error of each method as the global parameter estimation is robust to outliers. The parameters of the RANSAC were identical for all the methods and samples.</em><sup><a href="#references">[2]</a></sup>
<img src="img/ransac.png" alt="drawing" width="500"/></li>
</ul>
</li>
</ul>
<p><br></p>
<ul>
<li><span style="color: #00000; font-family: Babas; font-size: 1.25em;">Cascaded model refinement</span><sup><a href="#references">[9]</a></sup>:</li>
</ul>
<p><em>Additional GANs can be further utilized to refine the outputs in a staged manner. Using the single RGB frame formulation as an example, a GAN is trained to map an RGB frame to a depth map. Next, we introduce a secondary GAN that maps the concatenation of the RGB frame and depth map estimate to a more refined depth map. In other words, the secondary GAN is trained on the concatenation of the inputs and the outputs from the primary GAN.</em>
<img src="img/cmr_gan.png" alt="drawing" width="400"/></p>
<p><em>Issues: Paper does not demonstrate effectiveness of the above architecture</em>.</p>
<p><br></p>
<ul>
<li>
<p><a href="http://delivery.acm.org/10.1145/2610000/2601223/a86-fanello.pdf?ip=138.96.200.116&amp;id=2601223&amp;acc=ACTIVE%20SERVICE&amp;key=7EBF6E77E86B478F%2EC083A567C83E14C8%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1541768143_4cc7e69e959933f30c3480cf0ac37ef3">"Learning to be a Depth Camera for Close-Range Human Capture and Interaction", Microsoft Research</a></p>
<p><pre><img src="img/paper1_1.png" alt="drawing" width="400"/><img src="img/paper1_2.png" alt="drawing" width="300"/></pre></p>
<p><em>We present a machine learning technique for estimating absolute, per-pixel depth using any conventional monocular 2D camera, with minor hardware modifications.  Our approach targets close-range human capture and interaction where dense 3D estimation of hands and faces is desired. We use hybrid classification-regression forests to learn how to map from near infrared intensity images to absolute, metric  depth  in  real-time.   We  demonstrate  a  variety  of  human-computer interaction and capture scenarios. Experiments show an accuracy that outperforms a conventional light fall-off baseline, and is comparable to high-quality consumer depth cameras, but with a dramatically reduced cost, power consumption, and form-factor.</em></p>
</li>
</ul>
<p><br></p>
<ul>
<li>
<p>ActiveStereoNet: End-to-End Seld-supervised Learning for Active Stereo Systems</p>
<ul>
<li>precise depth with subpixel precision of 1/30th of a pixel.</li>
<li>does not suffer from over-smoothing issues</li>
<li>preserves edges</li>
<li>handles occlusions</li>
<li>robust to noise and texture-less patches</li>
<li>invariant to illumination changes.</li>
<li>
<p>IR stereo camera pair is used, pseudorandom pattern projected, captures active illumination and passive light.</p>
</li>
<li>
<p>Avoid matching occluded pixels (causes oversmoothing, edge fattening)</p>
</li>
<li>New reconstruction loss based on LCN (local constrast normalization):<ul>
<li>removes low frequency components from passive IR</li>
<li>re-calibrates the strength of active pattern locally to account for fading of patterns with distance.</li>
</ul>
</li>
<li>Window-based loss aggregation with adaptive weights for each pixel<ul>
<li>increase discriminability and reduce the effect of local minima in the stereo cost function.</li>
</ul>
</li>
<li>
<p>Detect and omit occluded pixels in the images during loss computations.</p>
</li>
<li>
<p>Self-supervised vs supervised passive stereo:
    Read how self-supervised passive work.</p>
</li>
<li>
<p>Build-in stereo algorithms in cameras (Intel D400) uses a handcrafted binary descriptor (CENSUS) in combination with a semi-global matching scheme.
        - suffers from common stereo matching issues (edge fattening, quadratic error, occlusions, holes)</p>
</li>
<li></li>
</ul>
</li>
</ul>
<p><br></p>
<ul>
<li>
<p>Received two datasets</p>
</li>
<li>
<p>LS-Net?</p>
</li>
<li>Fab-net and face metrics from IJB</li>
</ul>
<h1 id="references">References</h1>
<p>[2] <a href="https://arxiv.org/pdf/1703.10131.pdf">Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation, Sela et al.</a></p>
<p>[9] <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w21/Lore_Generative_Adversarial_Networks_CVPR_2018_paper.pdf">Generative adversarial networks for depth map estimation from RGB video, Kin Gwn Lore et al.</a></p>
<p>[10] <a href="http://www.cns.nyu.edu/pub/lcv/wang03-preprint.pdf">Image Quality Assessment: From Error Visibility to Structural Similarity, Zhou Wang et al.</a></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="datasets/" class="btn btn-neutral float-right" title="Datasets">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
        <span style="margin-left: 15px"><a href="datasets/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme.js" defer></script>
      <script src="search/main.js" defer></script>

</body>
</html>

<!--
MkDocs version : 1.0.4
Build Date UTC : 2018-11-12 02:03:15
-->
