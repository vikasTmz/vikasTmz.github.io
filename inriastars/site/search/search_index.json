{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project Report - Internship [Datasets] [Codes] [ToDo] November Week 1 & 2 Two tracks for the project Generating Face Depth Images from IR dot patterns image [Read More]. Face analysis from raw data [Read More]. Retrained on binarized IR images Non-binarized IR input Training loss [Test output results (link)] Binarized IR input Training loss [Test output results (link)] Trained on binarized IR images with varying distance from camera [Test output results (link)] Tested on unseen pose and orientation Evaluation metrics RMSE : log 10 error : Structural similarity index (SSIM) [10] : While metrics such as MSE estimate absolute errors, SSIM is a perceptionbased metric that considers image degradation as perceived change in structural information, while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms. Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in a visual rendering of a scene. Using this metric, we retain the original 2D structure of the image (as opposed to using a vector notation) since SSIM is computed on windows of images. [9] Ground Truth Output SSIM error log 10 error RMSE 0.98 0.0174 9.0051 0.973 0.0205 9.5557 0.978 0.0103 5.1186 0.983 0.0122 3.9700 0.986 0.0100 3.0364 0.991 0.0043 5.7457 0.972 0.0152 8.3283 0.968 0.0181 9.3406 0.906 0.0856 30.3137 0.854 0.1186 30.8789 RANSAC : For a quantitative comparison, we used the first 200 subjects from the BU-3DFE dataset, which contains facial images aligned with ground truth depth images. Each method provides its own estimation for the depth image alongside a binary mask, representing the valid pixels to be taken into account in the evaluation. Obviously, since the problem of reconstructing depth from a single image is ill-posed, the estimation needs to be judged up to global scaling and transition along the depth axis. Thus, we compute these paramters using the Random Sample Concensus (RANSAC) approach, for normalizing the estimation according to the ground truth depth. This significantly reduces the absolute error of each method as the global parameter estimation is robust to outliers. The parameters of the RANSAC were identical for all the methods and samples. [2] \"Unsupervised Adversarial Depth Estimation using Cycle Generative Networks\" [11] Evaluation Metrics (from the above paper) : Given P the total number of pixels in the test set and , the estimated depth and ground truth depth values for pixel i, we have: (i) the mean relative error (abs rel): , (ii) the squared relative error (sq rel): , (iii) the root mean squared error (rmse): , (iv) the mean log 10 error (rmse log): (v) the accuracy with threshold t, i.e.the percentage of such that \"Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation\" [13] : Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach benefits from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Cascaded model refinement [9] : Additional GANs can be further utilized to refine the outputs in a staged manner. Using the single RGB frame formulation as an example, a GAN is trained to map an RGB frame to a depth map. Next, we introduce a secondary GAN that maps the concatenation of the RGB frame and depth map estimate to a more refined depth map. In other words, the secondary GAN is trained on the concatenation of the inputs and the outputs from the primary GAN. Issues: Paper does not demonstrate effectiveness of the above architecture . \"Learning to be a Depth Camera for Close-Range Human Capture and Interaction\", Microsoft Research [12] : We present a machine learning technique for estimating absolute, per-pixel depth using any conventional monocular 2D camera, with minor hardware modifications. Our approach targets close-range human capture and interaction where dense 3D estimation of hands and faces is desired. We use hybrid classification-regression forests to learn how to map from near infrared intensity images to absolute, metric depth in real-time. We demonstrate a variety of human-computer interaction and capture scenarios. Experiments show an accuracy that outperforms a conventional light fall-off baseline, and is comparable to high-quality consumer depth cameras, but with a dramatically reduced cost, power consumption, and form-factor. ActiveStereoNet: End-to-End Seld-supervised Learning for Active Stereo Systems precise depth with subpixel precision of 1/30th of a pixel. does not suffer from over-smoothing issues preserves edges handles occlusions robust to noise and texture-less patches invariant to illumination changes. IR stereo camera pair is used, pseudorandom pattern projected, captures active illumination and passive light. Avoid matching occluded pixels (causes oversmoothing, edge fattening) New reconstruction loss based on LCN (local constrast normalization): removes low frequency components from passive IR re-calibrates the strength of active pattern locally to account for fading of patterns with distance. Window-based loss aggregation with adaptive weights for each pixel increase discriminability and reduce the effect of local minima in the stereo cost function. Detect and omit occluded pixels in the images during loss computations. Self-supervised vs supervised passive stereo: Read how self-supervised passive work. Build-in stereo algorithms in cameras (Intel D400) uses a handcrafted binary descriptor (CENSUS) in combination with a semi-global matching scheme. - suffers from common stereo matching issues (edge fattening, quadratic error, occlusions, holes) Received two datasets Read more . LS-Net? Fab-net and face metrics from IJB References [2] Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation, Sela et al. [9] Generative adversarial networks for depth map estimation from RGB video, Kin Gwn Lore et al. [10] Image Quality Assessment: From Error Visibility to Structural Similarity, Zhou Wang et al. [11] Unsupervised Adversarial Depth Estimation using Cycled Generative Networks, Andrea Pilzer et al. [12] \"Learning to be a Depth Camera for Close-Range Human Capture and Interaction\", Microsoft Research [13] Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation, Dan Xu et al.","title":"Home"},{"location":"#project-report-internship","text":"[Datasets] [Codes] [ToDo]","title":"Project Report - Internship"},{"location":"#november","text":"","title":"November"},{"location":"#week-1-2","text":"","title":"Week 1 &amp; 2"},{"location":"#two-tracks-for-the-project","text":"Generating Face Depth Images from IR dot patterns image [Read More]. Face analysis from raw data [Read More]. Retrained on binarized IR images Non-binarized IR input Training loss [Test output results (link)] Binarized IR input Training loss [Test output results (link)] Trained on binarized IR images with varying distance from camera [Test output results (link)] Tested on unseen pose and orientation Evaluation metrics RMSE : log 10 error : Structural similarity index (SSIM) [10] : While metrics such as MSE estimate absolute errors, SSIM is a perceptionbased metric that considers image degradation as perceived change in structural information, while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms. Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in a visual rendering of a scene. Using this metric, we retain the original 2D structure of the image (as opposed to using a vector notation) since SSIM is computed on windows of images. [9] Ground Truth Output SSIM error log 10 error RMSE 0.98 0.0174 9.0051 0.973 0.0205 9.5557 0.978 0.0103 5.1186 0.983 0.0122 3.9700 0.986 0.0100 3.0364 0.991 0.0043 5.7457 0.972 0.0152 8.3283 0.968 0.0181 9.3406 0.906 0.0856 30.3137 0.854 0.1186 30.8789 RANSAC : For a quantitative comparison, we used the first 200 subjects from the BU-3DFE dataset, which contains facial images aligned with ground truth depth images. Each method provides its own estimation for the depth image alongside a binary mask, representing the valid pixels to be taken into account in the evaluation. Obviously, since the problem of reconstructing depth from a single image is ill-posed, the estimation needs to be judged up to global scaling and transition along the depth axis. Thus, we compute these paramters using the Random Sample Concensus (RANSAC) approach, for normalizing the estimation according to the ground truth depth. This significantly reduces the absolute error of each method as the global parameter estimation is robust to outliers. The parameters of the RANSAC were identical for all the methods and samples. [2] \"Unsupervised Adversarial Depth Estimation using Cycle Generative Networks\" [11] Evaluation Metrics (from the above paper) : Given P the total number of pixels in the test set and , the estimated depth and ground truth depth values for pixel i, we have: (i) the mean relative error (abs rel): , (ii) the squared relative error (sq rel): , (iii) the root mean squared error (rmse): , (iv) the mean log 10 error (rmse log): (v) the accuracy with threshold t, i.e.the percentage of such that \"Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation\" [13] : Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach benefits from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Cascaded model refinement [9] : Additional GANs can be further utilized to refine the outputs in a staged manner. Using the single RGB frame formulation as an example, a GAN is trained to map an RGB frame to a depth map. Next, we introduce a secondary GAN that maps the concatenation of the RGB frame and depth map estimate to a more refined depth map. In other words, the secondary GAN is trained on the concatenation of the inputs and the outputs from the primary GAN. Issues: Paper does not demonstrate effectiveness of the above architecture . \"Learning to be a Depth Camera for Close-Range Human Capture and Interaction\", Microsoft Research [12] : We present a machine learning technique for estimating absolute, per-pixel depth using any conventional monocular 2D camera, with minor hardware modifications. Our approach targets close-range human capture and interaction where dense 3D estimation of hands and faces is desired. We use hybrid classification-regression forests to learn how to map from near infrared intensity images to absolute, metric depth in real-time. We demonstrate a variety of human-computer interaction and capture scenarios. Experiments show an accuracy that outperforms a conventional light fall-off baseline, and is comparable to high-quality consumer depth cameras, but with a dramatically reduced cost, power consumption, and form-factor. ActiveStereoNet: End-to-End Seld-supervised Learning for Active Stereo Systems precise depth with subpixel precision of 1/30th of a pixel. does not suffer from over-smoothing issues preserves edges handles occlusions robust to noise and texture-less patches invariant to illumination changes. IR stereo camera pair is used, pseudorandom pattern projected, captures active illumination and passive light. Avoid matching occluded pixels (causes oversmoothing, edge fattening) New reconstruction loss based on LCN (local constrast normalization): removes low frequency components from passive IR re-calibrates the strength of active pattern locally to account for fading of patterns with distance. Window-based loss aggregation with adaptive weights for each pixel increase discriminability and reduce the effect of local minima in the stereo cost function. Detect and omit occluded pixels in the images during loss computations. Self-supervised vs supervised passive stereo: Read how self-supervised passive work. Build-in stereo algorithms in cameras (Intel D400) uses a handcrafted binary descriptor (CENSUS) in combination with a semi-global matching scheme. - suffers from common stereo matching issues (edge fattening, quadratic error, occlusions, holes) Received two datasets Read more . LS-Net? Fab-net and face metrics from IJB","title":"Two tracks for the project"},{"location":"#references","text":"[2] Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation, Sela et al. [9] Generative adversarial networks for depth map estimation from RGB video, Kin Gwn Lore et al. [10] Image Quality Assessment: From Error Visibility to Structural Similarity, Zhou Wang et al. [11] Unsupervised Adversarial Depth Estimation using Cycled Generative Networks, Andrea Pilzer et al. [12] \"Learning to be a Depth Camera for Close-Range Human Capture and Interaction\", Microsoft Research [13] Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation, Dan Xu et al.","title":"References"},{"location":"abstract/","text":"Our approach targets close-range human capture and interaction where dense 3D estimation of hands and faces is desired. We use hybrid classification-regression forests to learn how to map from near infrared intensity images to absolute, metric depth in real-time. We recall that active sensors flood the scenes with texture and the intensity of the received signal follows the inverse square law I \u221d 1/Z^2 , where Z is the distance from the camera. In practice this creates an explicit dependency between the intensity and the distance (i.e. brighter pixels are closer). A second issue, that is also present in RGB images, is that the difference between two bright pixels is likely to have a bigger residual when compared to the difference between two dark pixels Depth cues are essential to achieving high-level scene understanding, and in particular to determining geometric relations between objects. The ability to reason about depth information in scene analysis tasks can often result in improved decision-making capabilities.Unfortunately, depth-capable sensors are not as ubiquitous as traditional RGB cameras, which limits the availability of depth-related cues.","title":"Abstract"},{"location":"codes/","text":"","title":"Codes"},{"location":"datasets/","text":"Datasets There is currently no dataset available containing IR Dot Patterns (IDP) of human faces and their corresponding depth images. There are several datasets containing RGB images and their corresponding depth images: There are also 3D face datasets: Therefore a dataset of IR dot pattern face images has to be created. Synthetic Datasets: [\u2713] For importance of IR patterns : Article and Paper [ ] Request other 3d face datasets and apply Multilinear Model Learning on them. Read Later : 3D reconstruction with glasses and Adaptive 3D Face Reconstruction from Unconstrained Photo Collections [ ] Use Diff. IR patterns [ ] Crop image using open CV. CromaKey for rgb images. (to be done when images are read for testing). Papers that use synthetic data for Face research Empirically Analyzing the Effect of Dataset Biases on Deep Face Recognition Systems Training Deep Face Recognition Systems with Synthetic Data Section III and IV, contains experiments using synthetic data for training and testing on in-the-wild face dataset. Use some of these points for Tuesday\u015b presentation. Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation 3D Face Reconstruction by Learning from Synthetic Data Real : Organise and plan data collection","title":"Datasets"},{"location":"datasets/#datasets","text":"There is currently no dataset available containing IR Dot Patterns (IDP) of human faces and their corresponding depth images. There are several datasets containing RGB images and their corresponding depth images: There are also 3D face datasets: Therefore a dataset of IR dot pattern face images has to be created.","title":"Datasets"},{"location":"datasets/#synthetic-datasets","text":"[\u2713] For importance of IR patterns : Article and Paper [ ] Request other 3d face datasets and apply Multilinear Model Learning on them. Read Later : 3D reconstruction with glasses and Adaptive 3D Face Reconstruction from Unconstrained Photo Collections [ ] Use Diff. IR patterns [ ] Crop image using open CV. CromaKey for rgb images. (to be done when images are read for testing).","title":"Synthetic Datasets:"},{"location":"datasets/#papers-that-use-synthetic-data-for-face-research","text":"Empirically Analyzing the Effect of Dataset Biases on Deep Face Recognition Systems Training Deep Face Recognition Systems with Synthetic Data Section III and IV, contains experiments using synthetic data for training and testing on in-the-wild face dataset. Use some of these points for Tuesday\u015b presentation. Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation 3D Face Reconstruction by Learning from Synthetic Data","title":"Papers that use synthetic data for Face research"},{"location":"datasets/#real","text":"Organise and plan data collection","title":"Real :"}]}