{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project Report - Internship [Datasets] [Codes] [ToDo] November Week 1 Two tracks for the project Generating Face Depth Images given RGB and IR image: Improving depth estimation of faces by providing both RGB and IR image to the GAN. [Results] This could be a Siamese network , with two channels to the generator. Experiment with different loss function. Introduce labels (gender, age etc.) to Discriminator in-order to improve results. Experiment with different IR-patterns. Evaluating Depth output (from RGB-DEPTH paper) Test ir2depth, binarized IR images, varied internsity and varied depth Use IR-GAN as training weights for Gender_CNN : Train Generator with discriminator using only labels and not depth. [ WISG\u201918 [1] [2] ] Face analysis from raw data: Add more layers to the CNN architecture / Transfer learning. Fab-net [learn more] Teacher student network. Retrained on binarized IR images Non-binarized IR input Training loss [Test output results (link)] Binarized IR input Training loss [Test output results (link)] Trained on binarized IR images with varying distance from camera [Test output results (link)] Tested on unseen pose and orientation Evaluation metrics RMSE : log 10 error : Structural similarity index (SSIM) [10] : While metrics such as MSE estimate absolute errors, SSIM is a perceptionbased metric that considers image degradation as perceived change in structural information, while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms. Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in a visual rendering of a scene. Using this metric, we retain the original 2D structure of the image (as opposed to using a vector notation) since SSIM is computed on windows of images. [9] Real Fake SSIM error log 10 error RMSE 0.98 0.0174 9.0051 0.973 0.0205 9.5557 0.978 0.0103 5.1186 0.983 0.0122 3.9700 0.986 0.0100 3.0364 0.991 0.0043 5.7457 0.972 0.0152 8.3283 0.968 0.0181 9.3406 0.906 0.0856 30.3137 0.854 0.1186 30.8789 RANSAC : For a quantitative comparison, we used the first 200 subjects from the BU-3DFE dataset, which contains facial images aligned with ground truth depth images. Each method provides its own estimation for the depth image alongside a binary mask, representing the valid pixels to be taken into account in the evaluation. Obviously, since the problem of reconstructing depth from a single image is ill-posed, the estimation needs to be judged up to global scaling and transition along the depth axis. Thus, we compute these paramters using the Random Sample Concensus (RANSAC) approach, for normalizing the estimation according to the ground truth depth. This significantly reduces the absolute error of each method as the global parameter estimation is robust to outliers. The parameters of the RANSAC were identical for all the methods and samples. [2] Cascaded model refinement [9] : Additional GANs can be further utilized to refine the outputs in a staged manner. Using the single RGB frame formulation as an example, a GAN is trained to map an RGB frame to a depth map. Next, we introduce a secondary GAN that maps the concatenation of the RGB frame and depth map estimate to a more refined depth map. In other words, the secondary GAN is trained on the concatenation of the inputs and the outputs from the primary GAN. Issues: Paper does not demonstrate effectiveness of the above architecture . \"Learning to be a Depth Camera for Close-Range Human Capture and Interaction\", Microsoft Research We present a machine learning technique for estimating absolute, per-pixel depth using any conventional monocular 2D camera, with minor hardware modifications. Our approach targets close-range human capture and interaction where dense 3D estimation of hands and faces is desired. We use hybrid classification-regression forests to learn how to map from near infrared intensity images to absolute, metric depth in real-time. We demonstrate a variety of human-computer interaction and capture scenarios. Experiments show an accuracy that outperforms a conventional light fall-off baseline, and is comparable to high-quality consumer depth cameras, but with a dramatically reduced cost, power consumption, and form-factor. ActiveStereoNet: End-to-End Seld-supervised Learning for Active Stereo Systems precise depth with subpixel precision of 1/30th of a pixel. does not suffer from over-smoothing issues preserves edges handles occlusions robust to noise and texture-less patches invariant to illumination changes. IR stereo camera pair is used, pseudorandom pattern projected, captures active illumination and passive light. Avoid matching occluded pixels (causes oversmoothing, edge fattening) New reconstruction loss based on LCN (local constrast normalization): removes low frequency components from passive IR re-calibrates the strength of active pattern locally to account for fading of patterns with distance. Window-based loss aggregation with adaptive weights for each pixel increase discriminability and reduce the effect of local minima in the stereo cost function. Detect and omit occluded pixels in the images during loss computations. Self-supervised vs supervised passive stereo: Read how self-supervised passive work. Build-in stereo algorithms in cameras (Intel D400) uses a handcrafted binary descriptor (CENSUS) in combination with a semi-global matching scheme. - suffers from common stereo matching issues (edge fattening, quadratic error, occlusions, holes) Received two datasets LS-Net? Fab-net and face metrics from IJB References [2] Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation, Sela et al. [9] Generative adversarial networks for depth map estimation from RGB video, Kin Gwn Lore et al. [10] Image Quality Assessment: From Error Visibility to Structural Similarity, Zhou Wang et al.","title":"Home"},{"location":"#project-report-internship","text":"[Datasets] [Codes] [ToDo]","title":"Project Report - Internship"},{"location":"#november","text":"","title":"November"},{"location":"#week-1","text":"","title":"Week 1"},{"location":"#two-tracks-for-the-project","text":"Generating Face Depth Images given RGB and IR image: Improving depth estimation of faces by providing both RGB and IR image to the GAN. [Results] This could be a Siamese network , with two channels to the generator. Experiment with different loss function. Introduce labels (gender, age etc.) to Discriminator in-order to improve results. Experiment with different IR-patterns. Evaluating Depth output (from RGB-DEPTH paper) Test ir2depth, binarized IR images, varied internsity and varied depth Use IR-GAN as training weights for Gender_CNN : Train Generator with discriminator using only labels and not depth. [ WISG\u201918 [1] [2] ] Face analysis from raw data: Add more layers to the CNN architecture / Transfer learning. Fab-net [learn more] Teacher student network. Retrained on binarized IR images Non-binarized IR input Training loss [Test output results (link)] Binarized IR input Training loss [Test output results (link)] Trained on binarized IR images with varying distance from camera [Test output results (link)] Tested on unseen pose and orientation Evaluation metrics RMSE : log 10 error : Structural similarity index (SSIM) [10] : While metrics such as MSE estimate absolute errors, SSIM is a perceptionbased metric that considers image degradation as perceived change in structural information, while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms. Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in a visual rendering of a scene. Using this metric, we retain the original 2D structure of the image (as opposed to using a vector notation) since SSIM is computed on windows of images. [9] Real Fake SSIM error log 10 error RMSE 0.98 0.0174 9.0051 0.973 0.0205 9.5557 0.978 0.0103 5.1186 0.983 0.0122 3.9700 0.986 0.0100 3.0364 0.991 0.0043 5.7457 0.972 0.0152 8.3283 0.968 0.0181 9.3406 0.906 0.0856 30.3137 0.854 0.1186 30.8789 RANSAC : For a quantitative comparison, we used the first 200 subjects from the BU-3DFE dataset, which contains facial images aligned with ground truth depth images. Each method provides its own estimation for the depth image alongside a binary mask, representing the valid pixels to be taken into account in the evaluation. Obviously, since the problem of reconstructing depth from a single image is ill-posed, the estimation needs to be judged up to global scaling and transition along the depth axis. Thus, we compute these paramters using the Random Sample Concensus (RANSAC) approach, for normalizing the estimation according to the ground truth depth. This significantly reduces the absolute error of each method as the global parameter estimation is robust to outliers. The parameters of the RANSAC were identical for all the methods and samples. [2] Cascaded model refinement [9] : Additional GANs can be further utilized to refine the outputs in a staged manner. Using the single RGB frame formulation as an example, a GAN is trained to map an RGB frame to a depth map. Next, we introduce a secondary GAN that maps the concatenation of the RGB frame and depth map estimate to a more refined depth map. In other words, the secondary GAN is trained on the concatenation of the inputs and the outputs from the primary GAN. Issues: Paper does not demonstrate effectiveness of the above architecture . \"Learning to be a Depth Camera for Close-Range Human Capture and Interaction\", Microsoft Research We present a machine learning technique for estimating absolute, per-pixel depth using any conventional monocular 2D camera, with minor hardware modifications. Our approach targets close-range human capture and interaction where dense 3D estimation of hands and faces is desired. We use hybrid classification-regression forests to learn how to map from near infrared intensity images to absolute, metric depth in real-time. We demonstrate a variety of human-computer interaction and capture scenarios. Experiments show an accuracy that outperforms a conventional light fall-off baseline, and is comparable to high-quality consumer depth cameras, but with a dramatically reduced cost, power consumption, and form-factor. ActiveStereoNet: End-to-End Seld-supervised Learning for Active Stereo Systems precise depth with subpixel precision of 1/30th of a pixel. does not suffer from over-smoothing issues preserves edges handles occlusions robust to noise and texture-less patches invariant to illumination changes. IR stereo camera pair is used, pseudorandom pattern projected, captures active illumination and passive light. Avoid matching occluded pixels (causes oversmoothing, edge fattening) New reconstruction loss based on LCN (local constrast normalization): removes low frequency components from passive IR re-calibrates the strength of active pattern locally to account for fading of patterns with distance. Window-based loss aggregation with adaptive weights for each pixel increase discriminability and reduce the effect of local minima in the stereo cost function. Detect and omit occluded pixels in the images during loss computations. Self-supervised vs supervised passive stereo: Read how self-supervised passive work. Build-in stereo algorithms in cameras (Intel D400) uses a handcrafted binary descriptor (CENSUS) in combination with a semi-global matching scheme. - suffers from common stereo matching issues (edge fattening, quadratic error, occlusions, holes) Received two datasets LS-Net? Fab-net and face metrics from IJB","title":"Two tracks for the project"},{"location":"#references","text":"[2] Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation, Sela et al. [9] Generative adversarial networks for depth map estimation from RGB video, Kin Gwn Lore et al. [10] Image Quality Assessment: From Error Visibility to Structural Similarity, Zhou Wang et al.","title":"References"},{"location":"abstract/","text":"Our approach targets close-range human capture and interaction where dense 3D estimation of hands and faces is desired. We use hybrid classification-regression forests to learn how to map from near infrared intensity images to absolute, metric depth in real-time. We recall that active sensors flood the scenes with texture and the intensity of the received signal follows the inverse square law I \u221d 1/Z^2 , where Z is the distance from the camera. In practice this creates an explicit dependency between the intensity and the distance (i.e. brighter pixels are closer). A second issue, that is also present in RGB images, is that the difference between two bright pixels is likely to have a bigger residual when compared to the difference between two dark pixels Depth cues are essential to achieving high-level scene understanding, and in particular to determining geometric relations between objects. The ability to reason about depth information in scene analysis tasks can often result in improved decision-making capabilities.Unfortunately, depth-capable sensors are not as ubiquitous as traditional RGB cameras, which limits the availability of depth-related cues.","title":"Abstract"},{"location":"codes/","text":"","title":"Codes"},{"location":"datasets/","text":"Datasets Data Generation Synthetic : [\u2713] Collect as many 3d face images (from 3d datasets) as possible maintaining annotations (gender, ethinicity, name?) along with them. [\u2713] Downloaded 2 face datasets with age, ethinicity, gender. [\u2713] pix2vertex : Issues: model needs to be smoothened, not robust to ethnicity . [\u2713] create image dataset with naming conventions and in separate directories [\u2713] For importance of IR patterns : Article and Paper [\u2713] Some pix2vertex output are not correct: filter them out by size (< 5MB), and store them to re-run. Also, track missing plys. Host dataset on website (rgb-d, ir dot pattern and 3d model) [ ] Request other 3d face datasets and apply Multilinear Model Learning on them. Read Later : 3D reconstruction with glasses and Adaptive 3D Face Reconstruction from Unconstrained Photo Collections [ ] Use Diff. IR patterns [ ] Crop image using open CV. CromaKey for rgb images. (to be done when images are read for testing). FaceGen: - [\u2713] Not to be used Papers that use synthetic data for Face research Empirically Analyzing the Effect of Dataset Biases on Deep Face Recognition Systems Training Deep Face Recognition Systems with Synthetic Data Section III and IV, contains experiments using synthetic data for training and testing on in-the-wild face dataset. Use some of these points for Tuesday\u015b presentation. Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation 3D Face Reconstruction by Learning from Synthetic Data Real : Organise and plan data collection","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/#data-generation","text":"","title":"Data Generation"},{"location":"datasets/#synthetic","text":"[\u2713] Collect as many 3d face images (from 3d datasets) as possible maintaining annotations (gender, ethinicity, name?) along with them. [\u2713] Downloaded 2 face datasets with age, ethinicity, gender. [\u2713] pix2vertex : Issues: model needs to be smoothened, not robust to ethnicity . [\u2713] create image dataset with naming conventions and in separate directories [\u2713] For importance of IR patterns : Article and Paper [\u2713] Some pix2vertex output are not correct: filter them out by size (< 5MB), and store them to re-run. Also, track missing plys. Host dataset on website (rgb-d, ir dot pattern and 3d model) [ ] Request other 3d face datasets and apply Multilinear Model Learning on them. Read Later : 3D reconstruction with glasses and Adaptive 3D Face Reconstruction from Unconstrained Photo Collections [ ] Use Diff. IR patterns [ ] Crop image using open CV. CromaKey for rgb images. (to be done when images are read for testing). FaceGen: - [\u2713] Not to be used","title":"Synthetic :"},{"location":"datasets/#papers-that-use-synthetic-data-for-face-research","text":"Empirically Analyzing the Effect of Dataset Biases on Deep Face Recognition Systems Training Deep Face Recognition Systems with Synthetic Data Section III and IV, contains experiments using synthetic data for training and testing on in-the-wild face dataset. Use some of these points for Tuesday\u015b presentation. Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation 3D Face Reconstruction by Learning from Synthetic Data","title":"Papers that use synthetic data for Face research"},{"location":"datasets/#real","text":"Organise and plan data collection","title":"Real :"}]}